{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Embeddings\n",
    "Taking our already generated embeddings form Cohere's Embed V3 Model, and training these embeddings in a hierarchical fashion. This will be achieved by sparse encoding the embeddings and integrating findings from the Matryoshka Embeddings Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse encoding will generate progressively simplified embeddings by reducing the dimensionality of the data by setting elements to zero based on a loss term. This means that we will be capturing the most important features in the lower dimensions (this is similar to the Matryoshka Idea), which mimics hierarchical embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most embeddings models, typically a loss function is applied on the full-size embedding to ensure the quality of the created embedding. However, in the Matryoshka framework, not only is the loss function applied to the full-size embeddings, but to dimensionally reduced (truncated) portions of the embeddings as well.\n",
    "\n",
    "\"Shortlisting and reranking: Rather than performing your downstream task (e.g., nearest neighbor search) on the full embeddings, you can shrink the embeddings to a smaller size and very efficiently \"shortlist\" your embeddings. Afterwards, you can process the remaining embeddings using their full dimensionality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers, backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Embeddings\n",
    "This may not be necessary since we will be using Cohere's Embed V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use cohere embed v3 - here will use a place holder for testing purposes\n",
    "\n",
    "#define the embeddings\n",
    "embeddings = np.random.rand(1000, 256) #example\n",
    "\n",
    "#instantiate the StandardScaler class\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#fit the scaler to the embeddings\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Sparse Autoencoder with Matryoshka Loss\n",
    "Following the Matryoshka framework, we will train a sparse autoencoder for different embedding dimensions. The goal is that the smallest dimensions perserve the most important data/features. Nested loss will be applied to penalize the lower-dimensional embeddings for losing important information.\n",
    "\n",
    "Sparse autoencoders create a compressed representation, enforcing sparsity via a loss term. Autoencoders consist of an encoder, which compresses the input into a lower-dimensional embedding; the decoser reconstructs the input from the compressed versions.\n",
    "\n",
    "In the encoding layer, an L1 regularization term is added as the sparsity constraint (it enforces sparsity on the embeddings (known as sparse codes)). L1 regularizationn works by encouraging many of the weights to be zero, which creates sparsity in the latent space (the 128-dim encoded space). As a result, this ensures that only a few dimensions carry signifiacnt informtion.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the input dimension\n",
    "input_dim = scaled_embeddings.shape[1]\n",
    "\n",
    "### defining the layers of the autoencoder ###\n",
    "\n",
    "#input_dim is 256; size of the original embedding\n",
    "input_layer = Input(shape = (input_dim,)) \n",
    "\n",
    "#compresses the input into a lower-dimensional representation; reduces dim to 128\n",
    "#regularizers.l1(1e-5) adds a sparsity constraint by penalizing large numbers of non-zero values in the embedding\n",
    "encoded = Dense(128, activation='relu', activity_regularizer=regularizers.l1(1e-5))(input_layer) \n",
    "\n",
    " #reconstructs dim back to 256\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "#building the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function will be modified so that it works across multiple embedding sizes. Here, we will use the mean squared error (MSE) for each dimension. The loss ensures that the smaller-scaled embeddings capture the most important/essential info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Matryoshka loss\n",
    "#assuming for this example that we are looking at the following dimensions: 64, 128, 256\n",
    "# ensures that lower-dimensional embeddings retain the most important info\n",
    "\n",
    "def matryoshka_loss(y_true, y_pred):\n",
    "    # Loss for 64-dimensional embedding\n",
    "    mse_64 = K.mean(K.square(y_true[:, :64] - y_pred[:, :64]))\n",
    "\n",
    "    # Loss for 128-dimensional embedding\n",
    "    mse_128 = K.mean(K.square(y_true[:, :128] - y_pred[:, :128]))\n",
    "\n",
    "    # Loss for 264-dimensional embedding\n",
    "    mse_256 = K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "    # Combine losses with increasing weight for larger dimensions\n",
    "    #combine losses with different weights to ensure that smaller dimensions are prioritized\n",
    "    return mse_64 + mse_128 * 0.5 + mse_256 * 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the autoencoder with the customized loss function, and using Adam as the optimizer\n",
    "autoencoder.compile(optimizer='adam', loss=matryoshka_loss)\n",
    "\n",
    "#building the encoder to extract sparse codes\n",
    "encoder = Model(input_layer, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder\n",
    "Training the autoencoder on Cohere's Embed v3 model. The sparsity constraint forces the model to focus on the most important features. The autoencoder is trained by minimizing the Matryoshka loss using the Adam optimizer. We train it in an unsupervised fashion by using the generated embeddings as the input and the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 2.3412 - val_loss: 2.2554\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.2522 - val_loss: 2.1737\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.1525 - val_loss: 2.1053\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0937 - val_loss: 2.0495\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0265 - val_loss: 2.0052\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.9883 - val_loss: 1.9709\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9567 - val_loss: 1.9447\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9274 - val_loss: 1.9249\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9136 - val_loss: 1.9100\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8943 - val_loss: 1.8985\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.8866 - val_loss: 1.8896\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8746 - val_loss: 1.8825\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.8667 - val_loss: 1.8767\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8600 - val_loss: 1.8719\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.8569 - val_loss: 1.8676\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8543 - val_loss: 1.8639\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8416 - val_loss: 1.8605\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8399 - val_loss: 1.8573\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8373 - val_loss: 1.8543\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8247 - val_loss: 1.8514\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8230 - val_loss: 1.8484\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8125 - val_loss: 1.8455\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8077 - val_loss: 1.8425\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8033 - val_loss: 1.8396\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7966 - val_loss: 1.8366\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7956 - val_loss: 1.8336\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7911 - val_loss: 1.8304\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7837 - val_loss: 1.8271\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7806 - val_loss: 1.8238\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7638 - val_loss: 1.8206\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7660 - val_loss: 1.8172\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7576 - val_loss: 1.8140\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7508 - val_loss: 1.8106\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7479 - val_loss: 1.8071\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7429 - val_loss: 1.8036\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7389 - val_loss: 1.7999\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7316 - val_loss: 1.7962\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7209 - val_loss: 1.7925\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7197 - val_loss: 1.7889\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7125 - val_loss: 1.7852\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7028 - val_loss: 1.7815\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7012 - val_loss: 1.7777\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.6924 - val_loss: 1.7739\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6891 - val_loss: 1.7701\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6826 - val_loss: 1.7664\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6769 - val_loss: 1.7627\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6721 - val_loss: 1.7590\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6697 - val_loss: 1.7553\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6630 - val_loss: 1.7517\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6605 - val_loss: 1.7480\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6534 - val_loss: 1.7443\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6465 - val_loss: 1.7406\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6407 - val_loss: 1.7369\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6383 - val_loss: 1.7333\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6327 - val_loss: 1.7298\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6261 - val_loss: 1.7261\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6213 - val_loss: 1.7225\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6149 - val_loss: 1.7191\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6116 - val_loss: 1.7156\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6058 - val_loss: 1.7122\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.6023 - val_loss: 1.7087\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5928 - val_loss: 1.7051\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5923 - val_loss: 1.7016\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5906 - val_loss: 1.6982\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5842 - val_loss: 1.6949\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5754 - val_loss: 1.6916\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5708 - val_loss: 1.6884\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5728 - val_loss: 1.6853\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5636 - val_loss: 1.6821\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5606 - val_loss: 1.6790\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5554 - val_loss: 1.6759\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5514 - val_loss: 1.6730\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5508 - val_loss: 1.6701\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5404 - val_loss: 1.6672\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5404 - val_loss: 1.6645\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5366 - val_loss: 1.6617\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5292 - val_loss: 1.6589\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5320 - val_loss: 1.6561\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5262 - val_loss: 1.6534\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5225 - val_loss: 1.6506\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5148 - val_loss: 1.6480\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5084 - val_loss: 1.6454\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5126 - val_loss: 1.6429\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5112 - val_loss: 1.6405\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5000 - val_loss: 1.6381\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5037 - val_loss: 1.6358\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5018 - val_loss: 1.6334\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4938 - val_loss: 1.6310\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4947 - val_loss: 1.6287\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4920 - val_loss: 1.6264\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4856 - val_loss: 1.6242\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4816 - val_loss: 1.6220\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4806 - val_loss: 1.6197\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4775 - val_loss: 1.6175\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4769 - val_loss: 1.6155\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.4718 - val_loss: 1.6135\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4703 - val_loss: 1.6115\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.4705 - val_loss: 1.6096\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4636 - val_loss: 1.6076\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4627 - val_loss: 1.6057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x264e7702690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(scaled_embeddings, scaled_embeddings,\n",
    "                epochs=100, #100 training cycles\n",
    "                batch_size=256,#256 embeddings per training step\n",
    "                shuffle=True, # randomized the data each epoch for better generalization\n",
    "                validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Sparse Codes\n",
    "Once the autoencoder is trained, we will use the trained encoder part of the model to extract sparse codes from the embeddings, which are the compressed 128-dimensional representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "#extracting sparse codes from the scaled embeddings \n",
    "sparse_codes = encoder.predict(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Decomposition\n",
    "We now have the sparse codes in 128 dimensions from the previous step. To create the hierarchical representations (aka the embeddings of different sizes), progressively truncate the sparse codes by removing the smallest components by setting them to zero. As it stands, the most important part of the data (most important features) are in the largest components. For each embedding vector, this function finds the top k largest absolute values and keeps them, setting the rest to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_sparse_codes(sparse_codes, k):\n",
    "    truncated_codes = np.copy(sparse_codes)\n",
    "    for i in range(truncated_codes.shape[0]):\n",
    "        threshold = np.sort(np.abs(truncated_codes[i]))[-k]\n",
    "        truncated_codes[i][np.abs(truncated_codes[i]) < threshold] = 0\n",
    "    return truncated_codes\n",
    "\n",
    "\n",
    "#example outputs \n",
    "#generating hierarchical embeddings at different scales\n",
    "# Generate hierarchical embeddings at different scales\n",
    "sparse_codes_64 = truncate_sparse_codes(sparse_codes, 64)\n",
    "sparse_codes_128 = truncate_sparse_codes(sparse_codes, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Codes dim = 64:  [[0.         0.         2.3489363  ... 0.         3.806964   1.6243358 ]\n",
      " [0.         0.         1.2997916  ... 0.         0.96140766 2.964773  ]\n",
      " [0.         0.         0.         ... 0.         1.8880342  0.01070255]\n",
      " ...\n",
      " [0.         0.         0.39064673 ... 0.         0.         2.8452244 ]\n",
      " [1.2399896  0.         1.6777012  ... 1.0786636  1.3557804  0.        ]\n",
      " [2.297599   3.1833317  0.         ... 0.         2.6793904  1.6575242 ]]\n",
      "Sparse Codes dim = 128:  [[0.         0.         2.3489363  ... 0.         3.806964   1.6243358 ]\n",
      " [0.         0.         1.2997916  ... 0.         0.96140766 2.964773  ]\n",
      " [0.         0.         0.         ... 0.         1.8880342  0.01070255]\n",
      " ...\n",
      " [0.         0.         0.39064673 ... 0.         0.         2.8452244 ]\n",
      " [1.2399896  0.         1.6777012  ... 1.0786636  1.3557804  0.        ]\n",
      " [2.297599   3.1833317  0.         ... 0.         2.6793904  1.6575242 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparse Codes dim = 64: \", sparse_codes_64)\n",
    "print(\"Sparse Codes dim = 128: \", sparse_codes_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Can perform several tasks to evaluate the quality of the hierarchical embeddings. Such examples include: performing classification tasks, and comparing the F1/accuracy scores of the embeddings of different dimensions; applying clustering algorithms on the embeddings and evaluate the qulaity using metrics like Silhouette Score or ARI; evaluating how well the different dimensional embeddings capture semantic similarity by comparing them using cosine similarity between different embeddings for similar data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between the two embeddings 64: 0.26770779\n",
      "Cosine Similarity between the two embeddings 128: 0.26770779\n"
     ]
    }
   ],
   "source": [
    "#Similarity Search using cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare cosine similarity between embeddings\n",
    "similarity_64 = cosine_similarity([sparse_codes_64[0]], sparse_codes_64[1:])\n",
    "similarity_128 = cosine_similarity([sparse_codes_128[0]], sparse_codes_128[1:])\n",
    "\n",
    "# Cosine similarity returns a 1x1 matrix, so we extract the single value to get the similarity score\n",
    "embedding_1_64 = sparse_codes_64[0].reshape(1, -1)  # First embedding\n",
    "embedding_2_64 = sparse_codes_64[1].reshape(1, -1)  # Second embedding\n",
    "similarity_64_score = cosine_similarity(embedding_1_64, embedding_2_64)[0][0]\n",
    "\n",
    "embedding_1_128 = sparse_codes_128[0].reshape(1, -1)  # First embedding\n",
    "embedding_2_128 = sparse_codes_128[1].reshape(1, -1)  # Second embedding\n",
    "similarity_128_score = cosine_similarity(embedding_1_128, embedding_2_128)[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity between the two embeddings 64: {similarity_64_score:.8f}\")\n",
    "print(f\"Cosine Similarity between the two embeddings 128: {similarity_128_score:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that the cosine similarity between two nearby points using the 64 dimensional and the 128 dimensional embeddings yield the same result. This demonstrates that pertinant information is not being lost by reducing the dimensionality of the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2677078  0.27551526 0.22697154 0.38289875 0.30744645 0.2544997\n",
      "  0.42064178 0.14423496 0.22994164 0.25471574 0.19051248 0.2027433\n",
      "  0.2717739  0.3316057  0.3397695  0.11698658 0.21989001 0.17641507\n",
      "  0.13010623 0.35663667 0.30376926 0.30757278 0.3958108  0.25582063\n",
      "  0.25202084 0.45863402 0.31988612 0.11422151 0.15509571 0.32597235\n",
      "  0.3244828  0.24448371 0.23829296 0.2867934  0.24815892 0.16879669\n",
      "  0.3062197  0.36652797 0.25208464 0.17279562 0.33739972 0.13860625\n",
      "  0.27460253 0.2649688  0.4103203  0.18800947 0.10189275 0.34627643\n",
      "  0.2883152  0.30326164 0.2454203  0.25379893 0.21497914 0.3021431\n",
      "  0.2837531  0.33348745 0.27229118 0.2903967  0.3477605  0.28745848\n",
      "  0.2013261  0.22856137 0.20963284 0.2641049  0.23649405 0.28403974\n",
      "  0.23301241 0.38936776 0.16701178 0.24890901 0.25155854 0.308977\n",
      "  0.30094    0.34131455 0.12677506 0.22907285 0.17504603 0.24738911\n",
      "  0.1147745  0.08729383 0.26229486 0.19662927 0.31852424 0.16724816\n",
      "  0.22927843 0.43393952 0.15565172 0.09541989 0.16737846 0.30529702\n",
      "  0.22966927 0.36627936 0.25014243 0.19632892 0.3359677  0.1922892\n",
      "  0.2632677  0.19615254 0.24174121 0.24527311 0.21061902 0.28934905\n",
      "  0.28140268 0.37068436 0.42157817 0.2849145  0.21368858 0.0845762\n",
      "  0.277923   0.23790993 0.1829423  0.18515712 0.33696407 0.27205265\n",
      "  0.22665754 0.17134061 0.13585643 0.18010825 0.3041197  0.21984598\n",
      "  0.2870409  0.29527473 0.28408757 0.138688   0.30359927 0.19839495\n",
      "  0.20870262 0.27409768 0.3648312  0.3608089  0.26137453 0.46335685\n",
      "  0.22975332 0.37031728 0.17395723 0.33240178 0.26165372 0.30210963\n",
      "  0.2218295  0.1448448  0.23022804 0.32552993 0.24657959 0.2881723\n",
      "  0.28242907 0.25493175 0.21437752 0.14656726 0.2612132  0.22999269\n",
      "  0.25262168 0.11900887 0.16430578 0.26676577 0.28692484 0.29281545\n",
      "  0.4096641  0.386487   0.18105765 0.20947304 0.17924193 0.26384753\n",
      "  0.29531366 0.1081068  0.1716811  0.23306857 0.16893837 0.4142414\n",
      "  0.1502806  0.25920254 0.26774618 0.17742456 0.23207638 0.24680829\n",
      "  0.2420482  0.24308193 0.21674418 0.28242743 0.15712516 0.27000976\n",
      "  0.18209586 0.34184915 0.27633137 0.1785086  0.22418739 0.3243558\n",
      "  0.21334425 0.14620808 0.27005765 0.13495222 0.24377939 0.29172647\n",
      "  0.19937322 0.37336308 0.24993756 0.24647787 0.30497432 0.15436672\n",
      "  0.25810862 0.23467635 0.33511984 0.22559252 0.18136078 0.41816548\n",
      "  0.23950066 0.287768   0.33393964 0.13385302 0.3445143  0.21342087\n",
      "  0.28225    0.4750296  0.13301653 0.39781213 0.33225387 0.15036443\n",
      "  0.23958986 0.27543566 0.28038538 0.19931157 0.308166   0.2465705\n",
      "  0.28844708 0.2595899  0.1452513  0.12600511 0.23610476 0.24993093\n",
      "  0.17345119 0.26718804 0.14359623 0.32025692 0.20191771 0.31574574\n",
      "  0.38153127 0.28187877 0.22150898 0.19713837 0.30489188 0.1658703\n",
      "  0.15526958 0.40680444 0.23283198 0.3527     0.12471973 0.22433391\n",
      "  0.2965557  0.2157318  0.16248477 0.16900375 0.20212771 0.18966441\n",
      "  0.37394297 0.2778056  0.42327943 0.30556756 0.15600327 0.20791464\n",
      "  0.2605229  0.25937575 0.09433328 0.29660076 0.2546187  0.32557613\n",
      "  0.2906929  0.3133104  0.37350297 0.3145219  0.4455321  0.23395094\n",
      "  0.215433   0.20899525 0.25156128 0.20848985 0.22572744 0.16141099\n",
      "  0.1400775  0.18824777 0.17068496 0.18363991 0.16580868 0.36974686\n",
      "  0.2262579  0.21571064 0.13405651 0.29486126 0.33947402 0.3662352\n",
      "  0.2678284  0.13163255 0.2152418  0.31688687 0.26586136 0.15018195\n",
      "  0.25188804 0.49692413 0.17242682 0.30177498 0.26583225 0.15591912\n",
      "  0.30107298 0.23099154 0.30656075 0.2585818  0.35155728 0.46942568\n",
      "  0.27664682 0.17782764 0.3337515  0.34711742 0.1702927  0.2872169\n",
      "  0.25495458 0.2726812  0.4241796  0.37717193 0.2720251  0.10777076\n",
      "  0.18513036 0.15022221 0.09830259 0.24035399 0.15350942 0.22308673\n",
      "  0.30663943 0.17027734 0.1765456  0.24883886 0.21351168 0.25507656\n",
      "  0.23034996 0.14356387 0.3423781  0.25502175 0.34170634 0.34412038\n",
      "  0.23845318 0.08855361 0.4095801  0.2527511  0.25738662 0.36705965\n",
      "  0.15719934 0.2691943  0.21868771 0.31662607 0.32733792 0.20540956\n",
      "  0.35516214 0.24207057 0.22227857 0.1379615  0.16601285 0.28001702\n",
      "  0.1985919  0.27203953 0.34276667 0.25117558 0.29316375 0.24895957\n",
      "  0.33543932 0.32775187 0.2590254  0.3128305  0.26837525 0.2491145\n",
      "  0.279323   0.32992545 0.23387966 0.31167272 0.2519842  0.20688832\n",
      "  0.1838568  0.18710777 0.25466746 0.21476358 0.2864358  0.22271511\n",
      "  0.11697812 0.2143631  0.23729473 0.21195596 0.18990992 0.24049678\n",
      "  0.1841084  0.33596766 0.22708559 0.39764416 0.21666586 0.20681374\n",
      "  0.10913655 0.24860263 0.20049208 0.28177983 0.20601016 0.12488704\n",
      "  0.24595717 0.22760579 0.19437088 0.18220516 0.41142043 0.3193922\n",
      "  0.4696467  0.43788797 0.29678226 0.30634156 0.07917447 0.3230653\n",
      "  0.23676425 0.2780016  0.16853088 0.20537975 0.21255156 0.10049005\n",
      "  0.13480283 0.15733925 0.31304562 0.2377643  0.23235127 0.24625218\n",
      "  0.32685792 0.32735062 0.18152161 0.22529472 0.19294359 0.35791373\n",
      "  0.2543047  0.07763314 0.17754306 0.39276138 0.35283083 0.196607\n",
      "  0.17366476 0.26688343 0.22478391 0.15535077 0.16955058 0.2339019\n",
      "  0.15025324 0.33081043 0.28836387 0.22472328 0.2466017  0.0895206\n",
      "  0.37020797 0.34685284 0.29306313 0.2610319  0.25526667 0.21874726\n",
      "  0.24043116 0.14559948 0.22590342 0.21153098 0.3117462  0.2585079\n",
      "  0.34755296 0.15748873 0.20680453 0.2899245  0.30032068 0.28218725\n",
      "  0.21368387 0.3791331  0.24246742 0.1956771  0.2568447  0.31807858\n",
      "  0.32472274 0.24093051 0.14556059 0.30751777 0.25906453 0.13327749\n",
      "  0.26282132 0.23050205 0.21325243 0.12819922 0.28410995 0.4388396\n",
      "  0.35804212 0.18868983 0.3652958  0.37046456 0.11183314 0.38157567\n",
      "  0.23889112 0.13991833 0.25321555 0.3122859  0.25477517 0.2469354\n",
      "  0.30496177 0.12605327 0.16991332 0.29094157 0.40865397 0.1426823\n",
      "  0.1610327  0.16890931 0.33995414 0.18403563 0.3216613  0.10072508\n",
      "  0.20981966 0.15024579 0.42927977 0.2028872  0.34257075 0.23005646\n",
      "  0.28006384 0.2568662  0.21609828 0.24780723 0.32505125 0.2950266\n",
      "  0.18342409 0.3390233  0.41634724 0.12802461 0.27704796 0.14987125\n",
      "  0.23233391 0.07564935 0.25367337 0.23051086 0.29096442 0.28075132\n",
      "  0.2936806  0.19856283 0.20428027 0.2456254  0.16953298 0.29617864\n",
      "  0.09382026 0.2226416  0.26860934 0.23968075 0.3724616  0.20700914\n",
      "  0.2922727  0.14874709 0.1529153  0.32508224 0.4073555  0.19277576\n",
      "  0.25604534 0.2072121  0.18323204 0.19145496 0.16913314 0.15583187\n",
      "  0.32402104 0.1008899  0.2201361  0.08537897 0.29730013 0.2674782\n",
      "  0.30496374 0.23808557 0.2839045  0.21001707 0.18678778 0.2598145\n",
      "  0.13843317 0.1078143  0.21163401 0.22510447 0.24282931 0.14201692\n",
      "  0.23107721 0.26623043 0.15442026 0.22401509 0.20338808 0.24371386\n",
      "  0.24669577 0.2218419  0.32643422 0.13243079 0.25161567 0.14272994\n",
      "  0.20708753 0.18441159 0.24285857 0.15099901 0.24683231 0.27390993\n",
      "  0.19109647 0.1637577  0.33845502 0.32002926 0.28062102 0.13553502\n",
      "  0.28009468 0.3118075  0.40850428 0.29235125 0.27576706 0.18507005\n",
      "  0.31104523 0.1724028  0.3122629  0.2563159  0.3081375  0.33979446\n",
      "  0.24339324 0.32539672 0.25215542 0.17823903 0.19893311 0.34692663\n",
      "  0.18055104 0.16041887 0.08639294 0.06873502 0.28503743 0.1242359\n",
      "  0.2256042  0.28158456 0.36003113 0.26668906 0.2793453  0.34841037\n",
      "  0.23350652 0.20128646 0.3026298  0.35118604 0.2939773  0.16644756\n",
      "  0.23568973 0.24129131 0.1394371  0.3329059  0.12625712 0.2197203\n",
      "  0.16582409 0.19338165 0.24953905 0.27524543 0.3972872  0.12416929\n",
      "  0.4478361  0.39708096 0.32572532 0.28400558 0.25484017 0.32267636\n",
      "  0.20302668 0.14741473 0.2923534  0.15049443 0.15623707 0.16011965\n",
      "  0.22221369 0.2772534  0.20297891 0.19995344 0.23105764 0.16918087\n",
      "  0.34249762 0.2893651  0.31352764 0.08107068 0.300579   0.42325753\n",
      "  0.13972527 0.16857213 0.2933647  0.1565081  0.28145579 0.29032296\n",
      "  0.37940702 0.20727608 0.3285618  0.15271765 0.16295075 0.14437991\n",
      "  0.08921744 0.27063152 0.1508614  0.31337753 0.2676382  0.14026996\n",
      "  0.25987038 0.32417575 0.3360443  0.35976183 0.3352549  0.319597\n",
      "  0.29599804 0.12360639 0.13041599 0.18703626 0.13368103 0.2773555\n",
      "  0.14009662 0.21946037 0.323957   0.14134285 0.19644189 0.20240445\n",
      "  0.22071946 0.19188002 0.2290583  0.21509662 0.23825905 0.24171892\n",
      "  0.26903573 0.23735285 0.27133873 0.24655439 0.36365014 0.35100567\n",
      "  0.19197062 0.08054136 0.3172465  0.27833486 0.19377406 0.15272708\n",
      "  0.34868616 0.30939597 0.24418075 0.26436794 0.27859855 0.23207575\n",
      "  0.25266874 0.23180768 0.20469914 0.23623098 0.22674501 0.21910733\n",
      "  0.296668   0.13166058 0.45037088 0.32014787 0.32855812 0.14969832\n",
      "  0.36811203 0.15795535 0.1260956  0.40940058 0.21294525 0.17245477\n",
      "  0.24787593 0.35806182 0.3408015  0.21628039 0.21264505 0.35602847\n",
      "  0.2019755  0.22507237 0.18973824 0.1514425  0.17658445 0.24082978\n",
      "  0.09482199 0.21397978 0.2842685  0.3064546  0.4154898  0.21557295\n",
      "  0.21379076 0.27697694 0.2573439  0.20406222 0.4261923  0.2972166\n",
      "  0.275959   0.23627816 0.21550034 0.21772598 0.2607882  0.21003166\n",
      "  0.22097962 0.15238057 0.20291501 0.26689154 0.29464093 0.18631214\n",
      "  0.2921695  0.2513628  0.29259562 0.3157694  0.2788249  0.1143576\n",
      "  0.17223997 0.26305538 0.2007192  0.33664346 0.39943594 0.3100388\n",
      "  0.28254938 0.3170448  0.22074959 0.25496557 0.28666717 0.20998684\n",
      "  0.18759666 0.33156577 0.1915949  0.25266907 0.3418037  0.20232679\n",
      "  0.3343202  0.234783   0.16219926 0.17464033 0.2570724  0.2760017\n",
      "  0.33417803 0.2720206  0.04383477 0.34931684 0.36136234 0.3143303\n",
      "  0.17922348 0.36944473 0.2571786  0.32108873 0.28915793 0.2783321\n",
      "  0.32559344 0.27017942 0.337895   0.24661729 0.21700999 0.20602907\n",
      "  0.29506257 0.20720088 0.30231977 0.4005623  0.25136855 0.32897258\n",
      "  0.29949188 0.30925655 0.15792322 0.32231465 0.25645867 0.4262067\n",
      "  0.23938292 0.24550399 0.27681407 0.21077017 0.260018   0.30486092\n",
      "  0.18806344 0.3094321  0.22141191 0.33997068 0.20674819 0.18010777\n",
      "  0.29144615 0.18208273 0.3032456  0.35909986 0.30191725 0.36377835\n",
      "  0.24598503 0.20992461 0.2914999  0.3426689  0.20492217 0.26769423\n",
      "  0.24408814 0.40179944 0.15384234 0.23177338 0.24173602 0.30293673\n",
      "  0.279036   0.39861256 0.2450276  0.395445   0.30037087 0.31677616\n",
      "  0.21594115 0.2422598  0.18323502 0.27874872 0.19492358 0.32052004\n",
      "  0.2626548  0.25561735 0.27888638 0.29512876 0.17144534 0.35620436\n",
      "  0.36573613 0.24817455 0.42945945 0.28877282 0.43701747 0.31617218\n",
      "  0.43002045 0.3876323  0.27584624 0.18791309 0.30782837 0.2951612\n",
      "  0.2571693  0.46256506 0.09365768 0.31154323 0.29575235 0.2614029\n",
      "  0.5104396  0.3531376  0.12943195 0.29244617 0.20541573 0.41352436\n",
      "  0.35470897 0.14378506 0.22461902 0.33488894 0.23676449 0.2762156\n",
      "  0.40792686 0.29434592 0.3403231  0.2278178  0.20918234 0.20597881\n",
      "  0.34622103 0.31500202 0.18304753 0.19877762 0.35553044 0.41474247\n",
      "  0.30246997 0.32793105 0.15026434 0.1480247  0.3677082  0.36223358\n",
      "  0.14001414 0.3618098  0.20281428 0.4049199  0.28129986 0.26227853\n",
      "  0.2707082  0.27628443 0.26470053 0.2514156  0.21385887 0.26412857\n",
      "  0.19865447 0.18439892 0.35507655 0.4364861  0.28759766 0.28305012\n",
      "  0.20653713 0.28101492 0.34876448 0.30163562 0.20613709 0.27996063\n",
      "  0.31815898 0.27158713 0.29977247 0.17814612 0.30845696 0.1286828\n",
      "  0.38905862 0.29373974 0.26136065 0.31813598 0.26186457 0.24988268\n",
      "  0.3595235  0.38606367 0.21833988 0.38810533 0.3000577  0.38031858\n",
      "  0.39432368 0.31199175 0.40331692 0.2500549  0.21151218 0.14871396\n",
      "  0.20418552 0.27592924 0.15268701 0.30896083 0.2926889  0.27353933\n",
      "  0.12937772 0.26499653 0.28265637 0.27487546 0.28469372 0.22537124\n",
      "  0.46644923 0.2761991  0.31481922]]\n",
      "[[0.2677078  0.27551526 0.22697154 0.38289875 0.30744645 0.2544997\n",
      "  0.42064178 0.14423496 0.22994164 0.25471574 0.19051248 0.2027433\n",
      "  0.2717739  0.3316057  0.3397695  0.11698658 0.21989001 0.17641507\n",
      "  0.13010623 0.35663667 0.30376926 0.30757278 0.3958108  0.25582063\n",
      "  0.25202084 0.45863402 0.31988612 0.11422151 0.15509571 0.32597235\n",
      "  0.3244828  0.24448371 0.23829296 0.2867934  0.24815892 0.16879669\n",
      "  0.3062197  0.36652797 0.25208464 0.17279562 0.33739972 0.13860625\n",
      "  0.27460253 0.2649688  0.4103203  0.18800947 0.10189275 0.34627643\n",
      "  0.2883152  0.30326164 0.2454203  0.25379893 0.21497914 0.3021431\n",
      "  0.2837531  0.33348745 0.27229118 0.2903967  0.3477605  0.28745848\n",
      "  0.2013261  0.22856137 0.20963284 0.2641049  0.23649405 0.28403968\n",
      "  0.23301241 0.38936776 0.16701178 0.24890901 0.25155854 0.308977\n",
      "  0.30094    0.34131455 0.12677506 0.22907285 0.17504603 0.24738911\n",
      "  0.1147745  0.08729383 0.26229486 0.19662927 0.31852424 0.16724816\n",
      "  0.22927843 0.43393952 0.15565172 0.09541989 0.16737846 0.30529702\n",
      "  0.22966927 0.36627936 0.25014243 0.19632892 0.3359677  0.1922892\n",
      "  0.2632677  0.19615254 0.24174121 0.24527311 0.21061902 0.28934905\n",
      "  0.28140268 0.37068436 0.42157817 0.2849145  0.21368858 0.0845762\n",
      "  0.277923   0.23790993 0.1829423  0.18515712 0.33696407 0.27205265\n",
      "  0.22665754 0.17134061 0.13585643 0.18010825 0.3041197  0.21984598\n",
      "  0.2870409  0.29527473 0.28408757 0.138688   0.30359927 0.19839495\n",
      "  0.20870262 0.27409768 0.3648312  0.3608089  0.26137453 0.46335685\n",
      "  0.22975332 0.37031728 0.17395723 0.33240178 0.26165372 0.30210963\n",
      "  0.2218295  0.1448448  0.23022804 0.32552993 0.24657959 0.2881723\n",
      "  0.28242907 0.25493175 0.21437752 0.14656726 0.2612132  0.22999269\n",
      "  0.25262168 0.11900887 0.16430578 0.26676577 0.28692484 0.29281545\n",
      "  0.4096641  0.386487   0.18105765 0.20947304 0.17924193 0.26384753\n",
      "  0.29531366 0.1081068  0.1716811  0.23306857 0.16893837 0.4142414\n",
      "  0.1502806  0.25920254 0.26774606 0.17742456 0.23207638 0.24680829\n",
      "  0.2420482  0.24308193 0.21674418 0.28242743 0.15712516 0.27000976\n",
      "  0.18209586 0.34184915 0.27633137 0.1785086  0.22418739 0.3243558\n",
      "  0.21334425 0.14620808 0.27005765 0.13495222 0.24377939 0.29172647\n",
      "  0.19937322 0.37336308 0.24993756 0.24647787 0.30497432 0.15436672\n",
      "  0.25810862 0.23467635 0.33511984 0.22559252 0.18136078 0.41816548\n",
      "  0.23950066 0.287768   0.33393964 0.13385302 0.3445143  0.21342087\n",
      "  0.28225    0.4750296  0.13301653 0.39781213 0.33225387 0.15036443\n",
      "  0.23958986 0.27543566 0.28038538 0.19931157 0.308166   0.2465705\n",
      "  0.28844708 0.2595899  0.1452513  0.12600511 0.23610476 0.24993093\n",
      "  0.17345119 0.26718804 0.14359623 0.32025692 0.20191771 0.31574574\n",
      "  0.38153127 0.28187877 0.22150898 0.19713837 0.30489188 0.1658703\n",
      "  0.15526958 0.40680444 0.23283198 0.3527     0.12471973 0.22433391\n",
      "  0.2965557  0.2157318  0.16248477 0.16900375 0.20212771 0.18966441\n",
      "  0.37394297 0.2778056  0.42327943 0.30556756 0.15600327 0.20791464\n",
      "  0.2605229  0.25937575 0.09433328 0.29661775 0.2546187  0.32557613\n",
      "  0.2906929  0.3133104  0.37350297 0.3145219  0.4455321  0.23395094\n",
      "  0.215433   0.20899525 0.25156128 0.20848985 0.22572744 0.16141099\n",
      "  0.1400775  0.18824777 0.17068496 0.18363991 0.16580868 0.36974686\n",
      "  0.2262579  0.21571064 0.13405651 0.29486126 0.33955997 0.3662352\n",
      "  0.2678284  0.13163255 0.2152418  0.31688687 0.26586136 0.15018195\n",
      "  0.25188804 0.49692413 0.17242682 0.30177498 0.26583225 0.15591912\n",
      "  0.30107298 0.23099154 0.30656075 0.2585818  0.35155728 0.46942568\n",
      "  0.27664682 0.17782764 0.3337515  0.34711742 0.1702927  0.2872169\n",
      "  0.25495458 0.2726812  0.42439234 0.37717193 0.2720251  0.10777076\n",
      "  0.18513036 0.15022221 0.09830259 0.24035399 0.15350942 0.22308673\n",
      "  0.30663943 0.17027734 0.1765456  0.24883886 0.21351168 0.25507656\n",
      "  0.23034996 0.14356387 0.3423781  0.25502175 0.34170634 0.34412038\n",
      "  0.23845318 0.08855361 0.4095801  0.2527511  0.25738662 0.36705965\n",
      "  0.15719934 0.2691943  0.21868771 0.31662607 0.32733792 0.20540956\n",
      "  0.35516214 0.24207057 0.22227857 0.1379615  0.16601285 0.28001702\n",
      "  0.1985919  0.27203953 0.34276667 0.25117558 0.29316375 0.24895957\n",
      "  0.33543932 0.32775187 0.2590254  0.3128305  0.26837525 0.2491145\n",
      "  0.279323   0.32992545 0.23387966 0.31167272 0.2519842  0.20688832\n",
      "  0.1838568  0.18710777 0.25466746 0.21476358 0.2864358  0.22271511\n",
      "  0.11697812 0.2143631  0.23729473 0.21195596 0.18990992 0.24049678\n",
      "  0.1841084  0.33596766 0.22708559 0.39764416 0.21666586 0.20681374\n",
      "  0.10913655 0.24860263 0.20049208 0.28177983 0.20601016 0.12488704\n",
      "  0.24595717 0.22760579 0.19437088 0.18220516 0.41142043 0.3193922\n",
      "  0.4696467  0.43788797 0.29678226 0.30634156 0.07917447 0.3230653\n",
      "  0.23676425 0.2780016  0.16853088 0.20537975 0.21255156 0.10049005\n",
      "  0.13480283 0.15733925 0.31304562 0.2377643  0.23235127 0.24625218\n",
      "  0.32685792 0.32735062 0.18152161 0.22529472 0.19294359 0.35791373\n",
      "  0.2543047  0.07763314 0.17754306 0.39276138 0.35283083 0.196607\n",
      "  0.17366476 0.26688343 0.22478391 0.15535077 0.16955058 0.2339019\n",
      "  0.15025324 0.33081043 0.28836387 0.22472328 0.2466017  0.0895206\n",
      "  0.37020797 0.34685284 0.29306313 0.2610319  0.25526667 0.21874726\n",
      "  0.24043116 0.14559948 0.22590342 0.21153098 0.3117462  0.2585079\n",
      "  0.34755296 0.15748873 0.20680453 0.2899245  0.30032068 0.28218725\n",
      "  0.21368387 0.3791331  0.24246742 0.1956771  0.2568447  0.31807858\n",
      "  0.32472274 0.24093051 0.14556059 0.30751777 0.25906453 0.13327749\n",
      "  0.26282132 0.23050205 0.21325243 0.12819922 0.28410995 0.4388396\n",
      "  0.35804212 0.18868983 0.3652958  0.37046456 0.11183314 0.38157567\n",
      "  0.23889112 0.13991833 0.25321555 0.3122859  0.25477517 0.2469354\n",
      "  0.30496177 0.12605327 0.16991332 0.29094157 0.40865397 0.1426823\n",
      "  0.1610327  0.16890931 0.33995414 0.18403563 0.3216613  0.10072508\n",
      "  0.20981966 0.15024579 0.42927977 0.2028872  0.34257075 0.23005646\n",
      "  0.28006384 0.2568662  0.21609828 0.24780723 0.32505125 0.2950266\n",
      "  0.18342409 0.3390233  0.41634724 0.12802461 0.27704796 0.14987125\n",
      "  0.23233391 0.07564935 0.25367337 0.23051086 0.29096442 0.28075132\n",
      "  0.2936806  0.19856283 0.20428027 0.2456254  0.16953298 0.29617864\n",
      "  0.09382026 0.2226416  0.26860934 0.23968075 0.3724616  0.20700914\n",
      "  0.2922727  0.14874709 0.1529153  0.32508224 0.4073555  0.19277576\n",
      "  0.25604534 0.2072121  0.18323204 0.19145496 0.16913314 0.15583187\n",
      "  0.3240903  0.1008899  0.2201361  0.08537897 0.29730013 0.2674782\n",
      "  0.30496374 0.23808557 0.2839045  0.21001707 0.18678778 0.2598145\n",
      "  0.13843317 0.1078143  0.21163401 0.22510447 0.24282931 0.14201692\n",
      "  0.23107721 0.26623043 0.15442026 0.22401509 0.20338808 0.24371386\n",
      "  0.24669577 0.2218419  0.32643422 0.13243079 0.25161567 0.14272994\n",
      "  0.20708753 0.18441159 0.24285857 0.15099901 0.24683231 0.27390993\n",
      "  0.19109647 0.1637577  0.33845502 0.32002926 0.28062102 0.13553502\n",
      "  0.28009468 0.3118075  0.40850428 0.29235125 0.27576706 0.18507005\n",
      "  0.31104523 0.1724028  0.3122629  0.2563159  0.3081375  0.33979446\n",
      "  0.24339324 0.32539672 0.25215542 0.17823903 0.19893311 0.34692663\n",
      "  0.18055104 0.16041887 0.08639294 0.06873502 0.28503743 0.1242359\n",
      "  0.2256042  0.28158456 0.36003113 0.26668906 0.2793453  0.34841037\n",
      "  0.23350652 0.20128646 0.3026298  0.35118604 0.2939773  0.16644756\n",
      "  0.23568973 0.24129131 0.1394371  0.3329059  0.12625712 0.2197203\n",
      "  0.16582409 0.19338165 0.24953905 0.27524543 0.3972872  0.12416929\n",
      "  0.4478361  0.39708096 0.32572532 0.28400558 0.25484017 0.32267636\n",
      "  0.20302668 0.14741473 0.2923534  0.15049443 0.15623707 0.16011965\n",
      "  0.22221369 0.2772534  0.20297891 0.19995344 0.23105764 0.16918087\n",
      "  0.34249762 0.2893651  0.31352764 0.08107068 0.300579   0.42325753\n",
      "  0.13972527 0.16857213 0.2933647  0.1565081  0.28145579 0.29032296\n",
      "  0.37940702 0.20727608 0.3285618  0.15271765 0.16295075 0.14437991\n",
      "  0.08921744 0.27063152 0.1508614  0.31337753 0.2676382  0.14026996\n",
      "  0.25987038 0.32417575 0.3360443  0.35976183 0.3352549  0.319597\n",
      "  0.29599804 0.12360639 0.13041599 0.18703626 0.13368103 0.2773555\n",
      "  0.14009662 0.21946037 0.323957   0.14134285 0.19644189 0.20240445\n",
      "  0.22071946 0.19188002 0.2290583  0.21509662 0.23825905 0.24171892\n",
      "  0.26903573 0.23735285 0.27133873 0.24655439 0.36365014 0.35100567\n",
      "  0.19197062 0.08054136 0.3172465  0.27833486 0.19377406 0.15272708\n",
      "  0.34868616 0.30939597 0.24418075 0.26436794 0.27859855 0.23207575\n",
      "  0.25266874 0.23180768 0.20469914 0.23623098 0.22674501 0.21910733\n",
      "  0.296668   0.13166058 0.45037088 0.32014787 0.32855812 0.14969832\n",
      "  0.36857042 0.15795535 0.1260956  0.40940058 0.21294525 0.17245477\n",
      "  0.24787593 0.35806182 0.3408015  0.21628039 0.21264505 0.35602847\n",
      "  0.2019755  0.22507237 0.18973824 0.1514425  0.17658445 0.24082978\n",
      "  0.09482199 0.21397978 0.2842685  0.3064546  0.4154898  0.21557295\n",
      "  0.21379076 0.27697694 0.2573439  0.20406222 0.4261923  0.2972166\n",
      "  0.275959   0.23627816 0.21550034 0.21772598 0.2607882  0.21003166\n",
      "  0.22097962 0.15238057 0.20291501 0.26689154 0.29464093 0.18631214\n",
      "  0.2921695  0.2513628  0.29259562 0.3157694  0.2788249  0.1143576\n",
      "  0.17223997 0.26305538 0.2007192  0.33664346 0.39943594 0.3100388\n",
      "  0.28254938 0.3170448  0.22074959 0.25496557 0.28666717 0.20998684\n",
      "  0.18759666 0.33156577 0.19403905 0.25274724 0.34178528 0.20232679\n",
      "  0.3385692  0.234783   0.16219926 0.17464033 0.2570724  0.2760017\n",
      "  0.33417803 0.27211878 0.04383477 0.3510968  0.36136234 0.31609678\n",
      "  0.17957972 0.36944473 0.2572345  0.32108873 0.28915793 0.27882352\n",
      "  0.3255934  0.27017942 0.33799917 0.24661729 0.21700999 0.20600826\n",
      "  0.29539376 0.20771557 0.30284786 0.4005623  0.25136855 0.32897258\n",
      "  0.29949188 0.31047714 0.15792322 0.32231465 0.26217818 0.42620662\n",
      "  0.2406971  0.2467835  0.27681407 0.21077017 0.260018   0.30510342\n",
      "  0.19026041 0.3094321  0.22141191 0.3399659  0.20695195 0.18010777\n",
      "  0.29144615 0.19355524 0.3032456  0.35909986 0.30191725 0.36413938\n",
      "  0.24598503 0.20992461 0.2914999  0.3426689  0.20492217 0.26777178\n",
      "  0.24408814 0.40179944 0.15384234 0.23177338 0.24173602 0.3029329\n",
      "  0.27903736 0.39861256 0.2450276  0.39915836 0.30037087 0.31842464\n",
      "  0.21594115 0.2422598  0.18323502 0.27906668 0.19492358 0.32063922\n",
      "  0.26320198 0.25561735 0.2788863  0.29512876 0.17149976 0.35616013\n",
      "  0.36573613 0.2481591  0.42944378 0.28877282 0.4371571  0.31617117\n",
      "  0.4300327  0.39640942 0.27646142 0.188041   0.3083816  0.29569355\n",
      "  0.2571693  0.46530256 0.09365768 0.31154323 0.2959866  0.2614029\n",
      "  0.5104395  0.35695317 0.12943195 0.29244617 0.20541573 0.41352436\n",
      "  0.35552487 0.14378506 0.22461902 0.33488894 0.23676449 0.2762156\n",
      "  0.40799105 0.29434592 0.3403231  0.23215997 0.20918234 0.20597881\n",
      "  0.34622103 0.31500202 0.18304753 0.19877553 0.35553044 0.41474247\n",
      "  0.30246997 0.32793105 0.15073061 0.1480247  0.36794105 0.36415786\n",
      "  0.14001414 0.3618098  0.20285265 0.40578964 0.28129986 0.26227853\n",
      "  0.2707082  0.2765275  0.26470053 0.25138846 0.21385887 0.26412857\n",
      "  0.19865447 0.2022125  0.35507655 0.43702307 0.28760457 0.28305012\n",
      "  0.20653713 0.28257877 0.34876305 0.30163562 0.20615609 0.27996063\n",
      "  0.31815898 0.27158713 0.2997722  0.17814612 0.3110155  0.1286828\n",
      "  0.38905862 0.29373974 0.26136065 0.31813598 0.26186457 0.24988268\n",
      "  0.37178028 0.38606367 0.21833988 0.38810533 0.30032334 0.38031858\n",
      "  0.3946737  0.31199175 0.40331692 0.2500549  0.21705216 0.14871396\n",
      "  0.20418042 0.27592924 0.15268701 0.31065634 0.29301512 0.27353933\n",
      "  0.12937772 0.26499653 0.28292093 0.27487546 0.28469372 0.22537124\n",
      "  0.46695387 0.2761991  0.31669563]]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_64)\n",
    "print(similarity_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
